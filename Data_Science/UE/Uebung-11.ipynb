{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übung 11\n",
    "\n",
    "**Gruppenname:**\n",
    "\n",
    "*Name Ihrer Gruppe*\n",
    "\n",
    "Gehen Sie wie folgt vor:\n",
    "\n",
    "1. Bitte benennen Sie jetzt dieses Jupyter Notebook um (z.B. über `File -> Rename`):\n",
    "\n",
    "   Namensschema: `Gruppenname-X`. Ersetzen Sie \"X\" mit der oben angegebenen Nummer der Übung.\n",
    "\n",
    "   - Beispiel 1: Team REZA bearbeitet Übung 2. Das Notebook heißt: REZA-2.\n",
    "   - Beispiel 2: Sie sind keinem Team zugeordnet (nicht empfehlenswert) und bearbeiten Übung 2: Ihr Notebook heißt: Nachname-2.\n",
    "\n",
    "\n",
    "2. Tragen Sie Ihren Gruppennamen auch ganz oben bei \"Name Ihrer Gruppe\" ein.\n",
    "\n",
    "3. Bearbeiten Sie dann dieses Notebook. Vergessen Sie dabei nicht, das Jupyter Notebook regelmäßig zu speichern (z.B. über `Strg + S` oder über `File -> Save and Checkpoint`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Curse of Dimensionality\n",
    "\n",
    "In der Vorlesung haben Sie gelernt, dass nicht intuitive Phänomene in hochdimensionalen Räumen auf Sie warten. Diese Phänomene - oft zusammengefasst als *Fluch der Dimensionalität* bezeichnet - stehen erfolgreichen Machine Learning Projekten oft im Wege, in der eine endliche und meist vorgegebene Datenpunktanzahl den Feature-Raum abdecken muss. Daher haben Sie Dimensionsreduktionsverfahren kennengelernt, die uns dabei helfen können, Daten aus hochdimensionalen Räumen in niedrigdimensionalere Räume zu projizieren.\n",
    "\n",
    "Für Nächste-Nachbarn Modelle spielt die räumliche Nachbarschaft von Datenpunkten im Feature-Raum eine entscheidende Rolle. Dies trifft auch auf viele andere Lernmodelle aus der Denkschule der Analogisten zu. In dieser Übung werden Sie die *Curse of Dimensionality* untersuchen, in dem Sie sich die Abstände zwischen Datenpunkten in Feature-Räumen ansehen werden.\n",
    "\n",
    "Ich beschreibe Ihnen zunächst die übergeordneten Ziele, bevor ich Sie Schritt für Schritt durch diese Aufgabe führe.\n",
    "\n",
    "**Übergeordnete Ziele**\n",
    "* Sie erstellen eine Abbildung, die den mittleren euklidischen Abstand zwischen zwei zufällig gewählten Vektoren in einem $D$-dimensionalen Hyperkubus der Kantenlänge $1$ in Abhängigkeit der Dimension $D$ darstellt.\n",
    "* Sie erstellen eine Abbildung, die kürzeste Distanz (also die Distanz zum nächsten Nachbarn) normiert auf die mittlere Distanz zwischen zufällig gewählten Datenpunkten als Funktion der Dimension darstellt.\n",
    "* Sie interpretieren Ihre beiden Abbildungen hinsichtlich der Curse of Dimensionality und der Nutzung von Nächste-Nachbarn Modellen.\n",
    "\n",
    "**Ihre Aufgaben**\n",
    "\n",
    "(1) Schreiben Sie eine Funktion `get_distances`, die die zwei skalaren Werte $N$ (Datenpunktanzahl) und $D$ (Dimension) entgegennimmt. Erzeugen Sie in Ihrer Funktion zwei Datensätze mit je $N$ zufälligen Datenpunkte mit je $D$ Features (Merkmalen), indem Sie die Features [aus einer uniformen Verteilung](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html) aus dem Interval \\[0, 1\\] ziehen. Ihre beiden Datensätze `x1` und `x2` sind jeweils Matrizen mit Shape NxD.\n",
    "\n",
    "* Bestimmen Sie nun die [euklidische](https://numpy.org/doc/stable/reference/generated/numpy.power.html) [Distanz](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sqrt.html) zwischen jeweils einem Datenpunkt aus `x1` und einem Datenpunkt aus `x2`, so dass Sie $N$ Distanzen erhalten. Ermitteln Sie den Mittelwert aus diesen $N$ Distanzen und geben Sie ihn über Ihre Funktion `get_distances` zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Hier implementieren Sie Ihre Funktion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Wählen Sie $N=1000$ und ermitteln Sie mithilfe Ihrer Funktion für $D=1,\\ldots, 100$ die mittlere Distanz zwischen zwei zufällig gewählten Datenpunkten in einem $D$-dimensionalen Hyperkubus der Kantenlänge 1. Visualisieren Sie die mittlere Distanz in Abhängigkeit der Dimension $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Interpretieren Sie Ihre Abbildung aus dem vorherigen Schritt: Was beobachten Sie? (1-3 Sätze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) In Nächste-Nachbarn Modellen spielen die nächsten Nachbarn eines Datenpunktes bei der Klassifikation oder Regression eine entscheidende Rolle. Modifizieren Sie Ihre Funktion wie folgt: Bestimmen Sie die [kürzeste Distanz](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.min.html) von den $N$ Distanzen und geben Sie zusätzlich den Quotienten (kürzeste Distanz / mittlere Distanz) zurück. Erzeugen Sie eine neue Abbildung, in der Sie Ihren Quotienten gegen die Dimension $D$ auftragen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Interpretieren Sie Ihre Abbildung aus dem vorherigen Schritt. \n",
    "\n",
    "* Wie verhält sich die kürzeste Distanz zur mittleren Distanz zwischen zufälligen Datenpunkten, wenn die Dimension $D$ wächst? (1 Satz) \n",
    "* Was bedeutet Ihr Ergebnis für Nächste-Nachbarn Modelle und für alle Verfahren, bei denen die Ähnlichkeit (bzw. Distanz) von Datenpunkten eine entscheidende Rolle spielen? (1-3 Sätze)\n",
    "* Nächste Nachbarn im niedrigdimensionalen Raum liegen nah an dem Datenpunkt, für den wir einen nächsten Nachbarn suchen. Wir suchen also in einer lokalen Umgebung. Wie ändert sich diese Umgebung in hohen Dimensionen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  11.2 Weinqualitäten (PCA und NN)\n",
    "\n",
    "In der Übung 9.2 konnten Sie bereits Bekanntschaft mit einem klassischen Datensatz im Machine Learning machen, den wir jetzt weiter untersuchen werden. Die Weine dreier italienischer Landwirte wurden auf ihre chemische Zusammensetzung hin untersucht. Dabei entstand in den 80er Jahren ein Datensatz, der zu einem der Klassiker im Machine Learning zählt. Sie werden mit dem k-nächste-Nachbarn Modell anhand der chemischen Zusammensetzung (Features) vorhersagen, von welchem der drei Landwirte (Labels) der Wein stammt. Dabei werden Sie Ihre Vorhersagen einmal direkt auf den Features und einmal auf PCA-transformierten Features durchführen.\n",
    "\n",
    "**Ihre Daten**\n",
    "\n",
    "Ich habe Ihnen den Weindatensatz in zwei Teile geteilt: Der erste Datensatz ist der sogenannte Trainingsdatensatz, mithilfe dessen Sie ihr Modell bauen werden. Der zweite Datensatz ist der sogenannte Testdatensatz, auf dem Sie ihr Modell anwenden und testen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T09:09:29.891684Z",
     "start_time": "2019-07-28T09:09:27.937737Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import data\n",
    "column_names = ['Class label', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', \n",
    "                'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)\n",
    "df.columns = column_names\n",
    "\n",
    "# preprocess data: split in training and test sets\n",
    "X, y = df.iloc[:, 1:].values, df.iloc[:, 0].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=10)\n",
    "\n",
    "\n",
    "# Your data: \n",
    "# training set: X_train, y_train\n",
    "# test set:     X_test,  y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ihre Aufgaben**\n",
    "\n",
    "(1) Importieren Sie die Daten, indem Sie die obere Code-Zelle ausführen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Vergegenwärtigen Sie sich die Eigenschaften des Wein-Datensatzes: Welche Eigenschaften wurden für die Weine erfasst (Variable `column_names`)? Wie viele Weinproben wurden genommen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Nächste-Nachbar-Modelle können empfindlich auf die Skalierung der Daten reagieren. In manchen Situationen ist es erwünscht, dass unterschiedliche Features unterschiedlich skaliert vorliegen (und damit meist in unterschiedlichen Einheiten angegeben werden). In vielen Fällen wollen wir aber, dass alle Features gleich skaliert vorliegen.\n",
    "\n",
    "* Untersuchen Sie also zunächst, ob die Daten standardisiert vorliegen: Haben alle Features denselben Mittelwert und dieselbe Standardabweichung?\n",
    "\n",
    "* Falls Sie die vorherige Frage mit Nein beantworten mussten, standardisieren Sie Ihre Features so, dass der Mittelwert jedes Features über den Datensatz hinweg den Wert \"0\" und die Standardabweichung \"1\" annimmt (dies entspricht einem sogenannten *z-scoring*). Schlagen Sie in der Vorlesung nach, wie Sie diese Skalierung umsetzen können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Wir werden jetzt ein kNN-Modell mit $k=1$ (Nächste Nachbarn Modell) erstellen. Schreiben Sie dazu eine Funktion mit dem Namen `NN`, die die standardisierten Trainingsdaten (`X_train` und `y_train`) sowie die standardisierten Testdaten (`X_test`) entgegen nimmt und die vorhergesagten Klassen (Labels) für die Testdaten als Vektor (`y_pred`) zurückgibt. Schlagen Sie in den Vorlesungsfolien nach, wie ein NN Modell definiert ist und nutzen Sie für die Implementierung numpy Funktionen.\n",
    "\n",
    "* Wenn Sie wenig Zeit haben, dann überspringen Sie diesen Schritt und nutzen für die nachfolgenden Teilaufgaben die  [vorgefertigte Implementierung des kNN-Modells](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Ermitteln Sie die vorhergesagten Labels (`y_pred`) für Ihre Test-Daten mithilfe Ihrer Funktion aus dem vorherigen Schritt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Bestimmen Sie die Genauigkeit (\"Accuracy\") Ihres Klassifikators. Die Genauigkeit ist die Anzahl der korrekt vorhergesagten Klassenlabels, dividiert durch die Anzahl aller Datenpunkte, für die Vorhersagen gemacht wurden (= die Datenpunktanzahl im Testset). Notieren Sie die Accuracy Ihres Klassifikators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) Wir werden jetzt eine PCA **auf den Trainingsdaten** durchführen, um die Dimension des Feature-Raums zu reduzieren. Ihre Features müssen dafür skaliert vorliegen. Dies haben Sie in Schritt (3) bereits sichergestellt.\n",
    "\n",
    "* Lesen Sie zunächst in den Vorlesungsfolien nach, wie eine PCA durchgeführt wird.\n",
    "* Nennen Sie die Dimensionen, die Sie für Ihre Kovarianzmatrix $S$ erwarten, die Sie aus Ihren Daten berechnen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) Bestimmen Sie die Kovarianzmatrix $S$ aus Ihren skalierten Daten. Nutzen Sie dafür eine Numpy Funktion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(9) [Berechnen Sie die Eigenwerte](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html) (Array: `eig_vals`) sowie assoziierten Eigenvektoren (`eig_vecs`) Ihrer Kovarianzmatrix $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(10) Ein typischer Fehler ist es, anzunehmen, dass Ihre Eigenwerte sortiert vorliegen. Dies ist im Allgemeinen *nicht* der Fall. Daher sortieren Sie bitte die Eigenwerte in absteigender Größe. Nutzen Sie dafür [diesen Befehl](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html), um die Sortierung in eine separate Variable `idx` zu speichern, mit der Sie dann die Sortierung der Eigenwerte vornehmen.\n",
    "\n",
    "* Beachten Sie, dass der Ihnen angegebene Befehl nicht in absteigender Reihenfolge (*descending order*) sortieren kann. Sortieren Sie also zunächst aufsteigend und kehren Sie dann die Reihenfolge im resultierenden Array um. \n",
    "* Falls Sie glauben, dass Ihre Eigenwerte schon sortiert vorliegen, ohne dass Sie sortieren müssen, prüfen Sie dies nach, indem Sie sich die Variable `idx` anschauen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(11) Sortieren Sie nun die assoziierten Eigenvektoren, die Sie im Array `eig_vecs` gespeichert hatten.\n",
    "* Beachten Sie: Die Eigenvektoren liegen als Spalten in `eigen_vecs` vor, *nicht* als Zeilen. Das Verwechseln von Zeilen und Spalten ist eine typische Fehlerquelle.\n",
    "* Nutzen Sie `idx`, um die Spalten (also die Eigenvektoren) so zu sortieren, dass sie zu Ihren sortierten Eigenwerten passen. Dazu können Sie [Integer Array Indexing](https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing) einsetzen. Für Integer Array Indexing müssen Sie Ihr Array `idx` [in eine Python Liste umwandeln](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tolist.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(12) Schlagen Sie in der Vorlesung nach, wie die *Proportion of Variance Explained* (PVE) definiert ist. Bestimmen Sie die PVE als Funktion der PCA-Komponenten und visualisieren Sie sie.\n",
    "\n",
    "* Beschreiben Sie kurz den Verlauf der PVE in Ihrer Abbildung. Wie interpretieren Sie diesen Verlauf? (1-2 Sätze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(13) Wir werden nun die PCA als Dimensionsreduktionsmethode einsetzen, indem Sie sich für eine Anzahl an PCA-Richtungen entscheiden, die etwas über 50% der gesamten Varianz der Daten enthalten. Betrachten Sie dazu Ihren PVE-Plot aus dem vorherigen Schritt. Wie viele der ersten PCA-Richtungen benötigen Sie, um etwas über 50% der gesamten Varianz der Daten zu erklären?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(14) Projizieren Sie Ihre normierten Daten auf die ersten PCA-Komponenten und erhalten Sie damit Ihre neuen Features: Beginnen Sie mit der ersten PCA-Komponente (also auf die Komponente mit dem größten Eigenvektor) und nennen Sie den resultierenden Vektor `x1`. Wiederholen Sie den Vorgang für die nächste PCA-Komponente und nennen Sie den resultierenden Vektor `x2`. Wiederholen Sie diesen Vorgang so lange, bis Sie alle neuen Koordinaten für die im vorherigen Schritt entschiedenen PCA-Komponenten erzeugt haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(15) Visualisieren Sie in einem Plot die Datenpunkte in den neuen, im Schritt (14) erzeugten, PCA-Komponenten und kodieren Sie farblich die Zugehörigkeit zur Weinklasse. Für die Farbkodierung kann Ihnen der Vektor `y_train` hilfreich sein. \n",
    "\n",
    "* Was fällt Ihnen auf? Lassen sich die verschiedenen Weine in diesem neuen Feature-Raum voneinander unterscheiden?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(16) Nutzen Sie die Eigenvektoren aus Schritt (11), um damit die *Testdaten* in ihre PCA-Komponenten zu projizieren.\n",
    "\n",
    "* Beachten Sie dabei Folgendes: Wir nutzen hier die PCA-Richtungen, die wir von den **Trainingsdaten** bestimmt haben, um damit die **Testdaten** zu transformieren. Wir nutzen damit dieselbe Transformation für die Trainings- wie für die Testdaten.\n",
    "\n",
    "* Welches Lernprinzip würde verletzt, wenn wir für die Ermittlung der PCA-Richtungen alle Daten verwenden würden (also Trainings- und Testdaten), um mit den dadurch gewonnenen PCA-Richtungen die Trainings- und die Testdaten zu transformieren? Schlagen Sie in den Vorlesungsfolien nach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "(17) Trainieren Sie ein neues NN Modell mithilfe der transformierten Trainingsdaten aus Schritt (14) und sagen Sie mithilfe Ihres trainierten Modells die Labels Ihres transformierten Testdatensatzes voraus. Bestimmen Sie - analog zu Schritt (6) - die Genauigkeit Ihres kNN-Klassifikators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(18) Vergleichen Sie die Genauigkeit Ihres Klassifikators aus Schritt (17) mit der aus Schritt (6): Was beobachten Sie? Welche Schlussfolgerung ziehen Sie? (1-3 Sätze)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
